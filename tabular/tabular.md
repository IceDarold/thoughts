Окей, давай спланируем так, будто ты приходишь в голую JupyterLab, только данные и стандартные библиотеки.

Формат: табличка + geo-фичи + эмбеддинги картинок. Цель — **гарантированно сделать сильный сабмит**, не умерев в перфекционизме.

Буду писать по тайм-слотам, с 11:00 до 18:00, с учётом обеда 13–14.

---

## 0:00–0:20 — Понять задачу и собрать скелет ноутбука

**Цели блока**: понять, что вообще оптимизируем, и не стрелять в пустоту.

1. **Прочитать условие:**

   * Что за таргет: регрессия / бинарка / multi-class?
   * Какая метрика: RMSE/MAE, AUC, logloss, MAP@K, что-то своё?
   * Какой формат сабмита: `id, target`? Несколько колонок? Пример сабмита.
2. **Создать один основной ноут**: `main_tabular_geo_image.ipynb`.

   * Секции: `0. Setup`, `1. Load data`, `2. Baseline`, `3. Features`, `4. Models`, `5. Submission`.
3. **Быстрый тех-чек:**

   * `import pandas, numpy, lightgbm, catboost, sklearn`.
   * `!ls` — какие файлы есть (`train.csv`, `test.csv`, эмбеддинги типа `image_embeddings.csv` и т.п.).
4. **Загрузить данные (быстро):**

   * `train = pd.read_csv(...)`, `test = pd.read_csv(...)`.
   * `train.head()`, `train.dtypes`, `train.shape`, `test.shape`.
   * Понять, где geo (lat/lon/region_id) и где эмбеддинги (много колонок `emb_0 ... emb_N`).

> На этом этапе ты уже должен понимать: размер данных, тип задачи, что за эмбеддинги, сколько фичей.

---

## 0:20–1:00 — Мини-EDA и самый простой baseline

**Цели блока**: быстро получить **первый валидный сабмит** и понять порядок метрики.

1. **Мини-EDA, совсем без фанатизма:**

   * Проверить пропуски: `train.isna().mean().sort_values().head(20)`.
   * Посмотреть таргет: `train[target].describe()` или `value_counts()` (для классификации).
   * Geo:

     * Если есть `lat/lon`: посмотреть диапазон, нет ли совсем странных значений.
     * Если есть `city/region_id`: посмотреть число уникальных.
   * Эмбеддинги:

     * Найти столбцы `emb_*`, посчитать их количество.
     * Проверить, нет ли NaN/Inf.
2. **Разделение train/val:**

   * Если есть явная группировка (например, `region_id`, `user_id`) → `GroupKFold(n_splits=5)`.
   * Если нет — `StratifiedKFold` для классификации или обычный `KFold` для регрессии.
3. **Очень простой baseline-модель:**

   * Фичи: **только табличные “обычные” фичи без эмбеддингов** (числовые + one-hot топовых категорий, остальное можно drop для старта).
   * Модель: LightGBM или CatBoost с дефолтными параметрами.
   * Сделать:

     * `fit` на train-fold, `predict` на val-fold, посчитать метрику.
     * Обучить на всём train и предсказать для test → `submission_v1.csv`.
4. **Сохранить baseline:**

   * Записать метрику по CV и сохранить сабмит (на всякий пожарный).

> К концу первого часа у тебя есть рабочий сабмит и референсная метрика. Жить можно.

---

## 1:00–1:40 — Нормальный CV и первые geo-фичи

**Цели блока**: сделать честный CV и показать ощутимый прирост за счёт гео.

1. **Оформить нормальную функцию обучения:**

   * `build_features_basic(df)` — пока только:

     * числовые фичи,
     * простая обработка категориальных (top-N levels → one-hot / CatBoost cat_features),
     * таргет не трогаем.
   * `train_cv_model(X, y, cv)` — возвращает oof, список моделей, среднюю метрику.
2. **Geo-фичи (версия 1):**

   * Если есть `lat/lon`:

     * перевести в радианы, сделать копии `lat_rad`, `lon_rad`;
     * нормализованный `lat_norm`, `lon_norm` (min–max или стандартизация).
   * Если есть `region/city`:

     * посчитать **count encoding** (сколько объектов в регионе);
     * можно добавить простую target-encoding по фолдам (аккуратно, без утечки).
3. **Прогнать CV с geo-фичами:**

   * Сравнить с baseline-метрикой.
   * Не тратить время на перебор гиперпараметров, кроме совсем базовых (n_estimators, lr, max_depth).

> К этому моменту у тебя: честный CV, +geo даёт прирост или нет — понятно.

---

## 1:40–2:00 — Подготовка «длинного» запуска на обед

**Цель блока**: перед 13:00 поставить тяжёлый прогон, который будет работать, пока ты ешь.

1. **Решить, что именно крутить на обеде:**

   * Вариант A: мощный LightGBM/CatBoost с чуть более глубокими деревьями, большим `n_estimators` и **полным набором табличных + geo-фичей**.
   * Вариант B: тот же, но уже с **эмбеддингами** (если успеваешь аккуратно подключить).
2. **Сделать минимальную интеграцию эмбеддингов (если не слишком больно):**

   * Найти эмбеддинг-таблицу (`image_emb.csv` или столбцы в train).
   * Убедиться, что `id` совпадает с train/test, правильно джоинится.
   * Добавить **самое простое**:

     * либо все `emb_*` как числовые фичи,
     * либо хотя бы 2–3 агрегата: `emb_norm`, `emb_mean`, `emb_max`.
3. **Подготовить код к долгому запуску:**

   * Оформить всё в одну функцию/ячейку:

     * построение фичей (табличные + geo + эмбеддинги, если уже завёл),
     * CV-обучение,
     * обучение на всём train,
     * предсказание на test,
     * сохранение сабмита `submission_lunch.csv`.
   * Проверить на **одном фолде** с `n_estimators сильно меньше`, что всё не падает.

---

## 2:00–2:10 (12:50–13:00 по факту) — Старт обеденного прогона

**Цель:** нажать Run и уйти.

1. Запускаешь большую ячейку с:

   * полным CV (4–5 фолдов),
   * адекватным `n_estimators` (с early stopping),
   * сохранением лучшей модели и сабмита в конец.
2. Убеждаешься, что:

   * лог метрики печатается,
   * ты не читаешь/не пишешь ничего интерактивного (чтобы не зависло на input).

**13:00–14:00 — Обед, модель считает, ты отдыхаешь.**

---

## 14:00–14:30 — Разбор результатов после обеда

**Цели блока**: понять, что дала «обеденная» модель, и какой из вариантов сейчас лидер.

1. **Посмотреть логи:**

   * средняя CV-метрика по фолдам;
   * были ли падения/ошибки;
   * сколько времени занял каждый фолд.
2. **Сравнить с baseline:**

   * Если прирост ощутимый — помечаешь этот вариант как текущий «best_pipeline».
   * Если не очень — фиксируешь, что именно не зашло (может, эмбеддинги «шумят»).
3. **Сделать быстрый sanity-check сабмита:**

   * размер совпадает с sample_submission,
   * нет NaN/Inf,
   * распределение таргета выглядит разумно.

---

## 14:30–15:15 — Нормальная интеграция эмбеддингов

Если ты эмбеддинги ещё толком не трогал — вот окно.

1. **Структура эмбеддингов:**

   * Количество измерений (например, 256/512/1024).
   * Проверка NaN, большие значения.
2. **Быстрая PCA (если размер большой):**

   * Взять только эмбеддинги: `emb_cols`.
   * `PCA(n_components=32/64)` → `emb_pca_*`.
   * В фичи подать только PCA-компоненты + пару агрегатов (норма, max).
3. **Модель с эмбеддингами:**

   * Тот же LightGBM/CatBoost, но с фичами `[tabular + geo + emb_pca + агрегаты]`.
   * Прогнать CV на 3–5 фолдах.
4. **Сравнить:**

   * Если с эмбеддингами лучше, чем без — поместить в «best_pipeline».
   * Если хуже/равно — оставить как запасной вариант, но не топить туда часы.

---

## 15:15–16:00 — Точечные улучшения: гео и регуляризация

**Цели блока**: дешёвые по времени фичи, которые часто дают +метрику.

1. **Гео-фичи (версия 2):**

   * Если есть lat/lon:

     * расстояние до условного «центра»/среднего (`mean_lat/mean_lon`);
     * KMeans по координатам на k=10–30 → кластерный id как категориальная фича.
   * Если есть регионы:

     * target encoding по регионам по фолдам (аккуратно, без утечки).
2. **Регуляризация / упрощение модели:**

   * Для LightGBM:

     * `num_leaves` поменьше, `feature_fraction`/`bagging_fraction` чуть < 1.0,
     * проверить, не переобучается ли сильно на некоторых фолдах.
3. **Ещё один CV-прогон лучшего варианта**:

   * Собрать «комбо»: табличные + geo_v2 + (опционально) PCA-эмбеддинги.
   * Посмотреть CV и стабильность по фолдам.

---

## 16:00–16:40 — Ансамбль и финальный выбор

**Цели блока**: не изобретать сложный стэкинг, а аккуратно бленднуть 2–3 лучших варианта.

1. **Выбрать 2–3 сильные модели:**

   * Например:

     * M1: табличка+geo, без эмбеддингов.
     * M2: табличка+geo+PCA-эмбеддинги.
     * (Опционально) M3: другая модель (CatBoost vs LightGBM).
2. **Сделать простой блендинг:**

   * Для регрессии: среднее/взвешенное среднее предсказаний.
   * Для классификации: среднее по вероятностям, потом argmax или пороги.
   * Подбор весов по OOF (если успеваешь) или просто equal-weight, если времени мало.
3. **Сравнить OOF-метрику ансамбля vs лучшая одиночная модель.**

   * Если прирост есть — ансамбль становится финальным пайплайном.
   * Если прироста почти нет — можно оставить одну лучшую модель (проще и надёжнее).

---

## 16:40–17:20 — Финальный прогон на всём train и сабмиты

**Цели блока**: сделать **чистый финальный сабмит**, который можно воспроизвести.

1. **Оформить финальную функцию:**

   * `build_features(df)` — единое место, где создаются все нужные фичи.
   * `train_and_predict()`:

     * делает CV (минимум 3 фолда),
     * обучает финальные модели на всём train,
     * генерирует `pred_test`.
2. **Обучить финальный ансамбль/модель на всём train:**

   * Сохранить:

     * OOF-предсказания,
     * сводку по фолдам (для себя),
     * финальные веса бленда.
3. **Собрать `submission_final.csv`:**

   * Убедиться в порядке колонок и типах.

---

## 17:20–18:00 — QA, страховочный саб и «репродьюс» в мини-версии

**Цели блока**: не умереть от какого-нибудь кривого сабмита и сделать себе «историю».

1. **Sanity-чек сабмита:**

   * кол-во строк == кол-во строк в test,
   * нет NaN/Inf,
   * диапазон таргета логичный (нет −1e9 и т.п.).
2. **Страховочный саб:**

   * Сохранить ещё одну версию (например, без эмбеддингов, если вдруг они глючат).
3. **Мини-репродьюс:**

   * В ноутбуке сверху сделать секцию:

     * «Как воспроизвести»: 5–7 ячеек по шагам:

       1. Импорт + константы.
       2. Загрузка данных.
       3. Построение фичей.
       4. Обучение финальной модели.
       5. Генерация сабмита.
   * Пройтись по ним **ещё раз** (на укороченном варианте, если датасет большой — можно сделать подвыборку) и убедиться, что всё работает.

---

### Если начнёшь не успевать

Приоритет по важности:

1. **Формат и baseline-сабмит** (первые 1–1.5 часа).
2. **Честный CV + простые geo-фичи**.
3. **Эмбеддинги через PCA + 1 модель**.
4. **Ансамбли/тонкая калибровка** — только если жмёт время и есть запас.

То есть лучше иметь **одну** аккуратную модель с geo и, возможно, без эмбеддингов, чем три сырых варианта, где можно что-нибудь сломать.

---

Если хочешь, следующим шагом могу расписать прямо «скелет main_tabular_geo_image.ipynb» по ячейкам: какие функции где объявить, в каком порядке запускать, чтобы это всё лёгко повторялось на туре.

---

И маленькое служебное: часть файлов, которые ты когда-то загружал в прошлых сессиях, у меня уже не доступна (они протухают со временем). Если захочешь, чтобы я снова посмотрел какой-то старый PDF/CSV/ноутбук — их нужно будет перезалить.
