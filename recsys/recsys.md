Давай сделаем себе прям сценарий на 7 часов, с учётом, что с 13 до 14 ты не трогаешь клавиатуру, но железо можно грузить.

Предположения:

* Задача: рекомендации VK Video.
* Есть:

  * логи: `user_id, item_id, ts, watch_time / click / like ...`
  * таблица видео: `item_id, embedding` (например 256/512 float) + мета (`duration`, `category`, `is_short`, `language`, …)
* Метрика типа Recall@K / NDCG@K / MAP@K по следующему просмотру.

Нет никакого репо → делаем **одну папку + один главный ноут + 1–2 .py** через `%%writefile`.

---

## 11:00–11:20 — Разбор условия и формата

Цель: точно понять, **что** предсказываем и **как** это считается.

1. Внимательно читаешь условие, фиксируешь в блокноте:

   * что именно надо рекомендовать (следующее видео? топ-K ленты?),
   * какая метрика и её точное определение,
   * формат сабмита (CSV/JSON, поля: `user_id,item_id_1 ... item_id_K`).
2. Сразу замечаешь:

   * есть ли **sample_submission** и **локальный скрипт метрики**,
   * где лежат данные (аналог их `Shared`/`Permanent` — выпиши пути).
3. Создаёшь рабочую папку, напр. `vk_recs/`, внутри:

   * `main.ipynb` — единственный ноут,
   * `tmp/` — для промежуточных файлов (если надо).
4. В блокноте/markdown в ноуте пишешь план крупными блоками, чтобы не держать в голове.

---

## 11:20–11:45 — Поднятие данных и минимальная разведка

Цель: увидеть размеры и основные поля.

1. В `main.ipynb`:

   * импорт: `pandas`, `numpy`, `sklearn`, `lightgbm`/`catboost` (если есть), `faiss` (если есть), `tqdm`.
2. Загружаешь:

   * `interactions.csv` (или как называется),
   * `items.csv` / `videos.csv` с эмбеддингами и мета.
3. Быстрые проверки:

   * `df.shape`, число уникальных юзеров/видео,
   * какие есть таргеты (watch_time, click, like),
   * базовый `value_counts` по таргету, распределение по времени.
4. Проверяешь размер эмбеддинга (кол-во столбцов) и что там нет NaN.

Не увязаешь в EDA — только самое необходимое, чтобы понять структуру.

---

## 11:45–12:15 — Первый baseline: популярность

Цель: за 30 минут иметь **полностью рабочий пайплайн сабмита**.

1. Строишь **global popularity**:

   * агрегат по `item_id`: суммарный `watch_time` или количество взаимодействий за всё время (или за «хвост» по времени, если есть разметка).
2. Собираешь топ-K (например, K по метрике — 20/50).
3. Для каждого пользователя в test:

   * выдаёшь **один и тот же список топ-K** (или чуть модифицируешь под язык/категорию, если это тривиально).
4. Если есть **локальный скрипт метрики**:

   * сразу его оборачиваешь в функцию `evaluate(popular_submit)` и проверяешь, что всё считает.
5. Если нет скрипта:

   * хотя бы проверяешь формат сабмита: строки, типы, нет ли пропусков.

Итог: через 1.5 часа у тебя должен быть **валидный сабмит** и функция оценивания (или максимально приближённый аналог).

---

## 12:15–12:45 — Валидация и сплит

Цель: честный off-line скор, чтобы любые изменения мерить.

1. Выбираешь стратегию CV:

   * чаще всего **time-based**: для каждого юзера последние N взаимодействий → вал, остальные → train.
   * или leave-one-out: последнее взаимодействие на вал, всё до него на train.
2. Пишешь функцию:

   ```python
   def make_cv_split(interactions):
       # возвращает train_df, val_df (или список фолдов)
   ```
3. Реализуешь `evaluate_model(predict_fn, interactions, items)`:

   * генеришь рекомендации по пользователям (top-K),
   * считаешь метрику (Recall@K / MAP@K / NDCG@K).
4. Прогоняешь baseline (popular) через этот CV.

   * Фиксируешь число в ноуте и в комментарии: “Baseline_popular: Recall@20 = …”

Это твой «пол» метрики.

---

## 12:45–13:00 — Подготовка тяжёлых джобов на обед

Цель: поставить на обед то, что будет долго считаться.

Из того, что реально полезно:

1. Построить **item-item kNN по эмбеддингам**:

   * собрать матрицу `items_embeddings` (`n_items × d`),
   * подготовить FAISS-индекс (если библиотека есть), иначе — bruteforce батчами и сохранить top-N соседей для каждого item.
2. Либо запустить простой **matrix factorization / LightFM / implicit ALS** (если есть библиотека и формат позволяет):

   * подготовить sparse user-item матрицу,
   * запустить обучение на весь train.
3. В ноуте:

   * пишешь клетки с запуском (`fit()`/`index.train()+index.add()`/`save()`),
   * **запускаешь** и отходишь на обед.

---

## 13:00–14:00 — Обед, пока считает

Ты не трогаешь клавиатуру, но в идеале к возвращению у тебя уже:

* лежит готовый FAISS-индекс / файл `item_knn.parquet`,
* или обученная implicit/ALS модель,
* или хотя бы частично посчитанные структуры.

---

## 14:00–14:20 — Разбор результатов, выбор направления

Цель: быстро понять, что из крупняка сработало.

1. Смотришь:

   * получилось ли сохранить индекс / модель,
   * нет ли OOM/ошибок.
2. На минимальном сэмпле запускаешь:

   * рекомендации «похожих видео» по эмбеддингу (item-based),
   * рекомендации из MF/ALS по прошлому просмотру.
3. Прогоняешь через твою `evaluate_model` на одном фолде:

   * сравниваешь с popularity.
4. Выбираешь **основной кандидатный генератор**:

   * `popular + item_knn по последним просмотренным видео` или
   * `popular + ALS`.

---

## 14:20–15:00 — Нормальный candidate generation

Цель: собрать богатый список кандидатов на пользователя.

1. Определяешь, какую историю пользователя используешь:

   * последние N просмотров по времени (например, 20–50).
2. Для каждого пользователя:

   * **Source 1:** популярное (по сегменту: тип видео / язык / длительность).
   * **Source 2:** item-item по эмбеддингу — соседи для последних просмотренных.
   * **Source 3:** если есть ALS — его топ-N рекомендаций.
3. Объединяешь кандидатов:

   * берёшь union, убираешь дубликаты,
   * ограничиваешь размер кандидатов на пользователя (скажем 200–300).
4. Пишешь функцию:

   ```python
   def get_candidates_for_users(user_hist, items, models, Kcands=300):
       # возвращает dict user_id -> list[item_id]
   ```
5. Прогоняешь candidate-only (без ранкера):

   * ранжируешь просто по весу источника (popular_score / similarity_score),
   * меряешь метрику относительно чистого popularity.

---

## 15:00–15:40 — Фичи и первый ранкер

Цель: сделать двухэтапный pipeline: candidate → ranker.

1. Для каждой пары (user, candidate_item) считаешь фичи:

   * популярность item (глобальная, по последним дням),
   * recency: насколько item свежий,
   * user_stats: число просмотренных, avg watch_time, доля short videos,
   * content features:

     * cos-sim(user_profile_emb, item_emb), где user_profile_emb = среднее по emb просмотренных,
     * cos-sim(item_emb, last_item_emb),
   * если есть ALS/implicit: предсказанный score.
2. Собираешь обучающую выборку:

   * train interactions → для каждого `(user, positive_item)` выбираешь несколько `negative_items` из кандидатов.
   * target: бинарный (1/0) или ranking (pairwise).
3. Обучаешь LightGBM / CatBoost:

   * небольшой depth (6–8), 500–1000 деревьев,
   * простой train/val split = твой time-based фолд.
4. Прогоняешь через `evaluate_model`:

   * фиксируешь новую метрику: “Ranker_v1: Recall@20 = …”.

---

## 15:40–16:10 — Улучшение ранкера и правила

Цель: быстро добрать ещё пару пунктов метрики.

1. Добавляешь 2–5 новых фич:

   * доля совпадающих категорий user↔item,
   * log-популярность,
   * “position bias” (порядок кандидатов по одному из источников как фича).
2. Проверяешь:

   * feature importance,
   * нет ли ликов (например, фич, завязанных на будущие события).
3. Чуть подстраиваешь:

   * `num_leaves`, `min_data_in_leaf`,
   * learning_rate (но без глубокого HPO).
4. Снова `evaluate_model` на 1–2 фолдах:

   * если прирост >0.5–1 п.п. — оставляешь,
   * если нет — фиксируешь v1 как основную.

---

## 16:10–16:40 — Пост-процесс и cold-start

Цель: сделать решение аккуратным, устойчивым и дружелюбным к краям.

1. **Post-processing** на уровне рекомендаций:

   * гарантия **без дублей** в топ-K,
   * убрать из списка уже просмотренные пользователем (если по условию так надо),
   * лёгкая диверсификация: не более N видео одного автора/категории подряд.
2. **Cold users/items**:

   * Если у юзера почти нет истории → heavy weight на popularity и контент (по демо/языку, если есть).
   * Для item без истории → оценка только по content-фичам.
3. Проверяешь метрику ещё раз с post-process:

   * иногда лёгкая диверсификация почти не бьёт по метрике, а иногда — бьёт; если заметно падает, ослабляешь.

---

## 16:40–17:10 — Финальный конвейер и проверка времени

Цель: иметь единый, понятный, воспроизводимый путь “данные → сабмит”.

1. В ноуте делаешь секцию “**Full pipeline**”:

   * загрузка данных,
   * построение candidates,
   * генерация фич и ранкер,
   * генерация рекомендаций для test,
   * запись сабмита.
2. Замеряешь время прогона на полном train/test:

   * прикидываешь, сколько займёт “с нуля” в их окружении (должно вписываться в 1–2 часа).
3. Если что-то очень медленно:

   * сохраняешь промежуточные артефакты (item_knn, user_profile_emb, подготовленные фичи) на диск,
   * делаешь опцию “пересчитать / использовать кеш”.

---

## 17:10–17:40 — Репродьюс: прогон «с нуля»

Цель: убедиться, что жюри сможет запустить твой код и получить тот же сабмит.

1. Рестартуешь kernel / создаёшь новый ноутбук-витрину:

   * последовательно прогоняешь только финальные клетки.
2. Проверяешь:

   * никаких ручных правок,
   * все пути относительные,
   * сиды зафиксированы,
   * формат сабмита 1:1 с примером.
3. Сохраняешь:

   * финальный сабмит (имя типа `submission_final.csv`),
   * если можно — короткий `README` в markdown-ячейке (как запускать).

---

## 17:40–18:00 — Финиш и запасной вариант

Цель: иметь и **боевой**, и **резервный** вариант.

1. Делаешь **копию сабмита** (на всякий случай).
2. Фиксируешь:

   * baseline-метрику и финальную,
   * какие фичи/части дали самый большой прирост (на случай устного разбора).
3. Если вдруг что-то ломается в самом конце:

   * у тебя всегда есть рабочий baseline (popularity/простые кандидаты), который воспроизводим.

---

Если хочешь, дальше можем расписать уже не по времени, а по коду: какие именно функции/структуры я бы писал в `main.ipynb` (сигнатуры `make_cv_split`, `build_candidates`, `build_features`, `train_ranker`, `predict_for_test` и т.п.).

И маленькое служебное: часть файлов, которые ты раньше заливал (PDF с задачами и т.п.), у меня сейчас уже истекла и недоступна. Если захочешь, чтобы я снова их читал/цитировал — просто перезагрузи.
