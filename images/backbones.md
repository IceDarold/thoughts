# CNN-бэкбоны, которые реально полезны на туре  
(с конкретными моделями на Hugging Face)

Цель: не энциклопедия по всем CNN, а чек-лист «что имеет смысл реально использовать за 5–7 часов тура».

---

## 0. Что такое backbone и как мы его используем на туре

**Backbone** — это предобученный сверточный экстрактор признаков, который ты:

- либо **фризишь** и обучаешь только голову (MLP / линейный слой / LightGBM),
- либо **легко дообучаешь** (fine-tune) несколько верхних слоёв.

Типичные сценарии на олимпиаде:

1. Картинки → предобученный CNN → embedding → табличная модель / простая голова.
2. Аудио → лог-мел спектр → CNN как image-backbone.
3. Аудио → готовый audio-CNN (PANNs, VGGish, YamNet) → embedding → дальше табличка/MLP.

---

## 1. Vision CNN-бэкбоны (картинки + спектрограммы)

### 1.1 ResNet-семейство — «дефолтный молоток»

**Идея:** residual-блоки, стабильное обучение, предсказуемое поведение. Хороший баланс «качество/простота/скорость».

Полезные на HF:

- `microsoft/resnet-18` — маленький, быстрый, годится для baseline и маленьких датасетов. :contentReference[oaicite:0]{index=0}  
- `microsoft/resnet-50` — стандартный «рабочий конь» для картинок. :contentReference[oaicite:1]{index=1}  
- `timm/resnet50.tv_in1k` — та же идея, но из timm-мира (ResNet-B на ImageNet-1k). :contentReference[oaicite:2]{index=2}  

**Когда брать на туре:**

- Нужно **быстро получить сильный baseline** по изображениям или спектрограммам.
- Датасет **среднего размера** (тысячи–десятки тысяч примеров).
- Хочешь минимизировать риск «модель не сошлась/переобучилась».

**Плюсы:**

- Очень предсказуемый.
- Легко fine-tune, много примеров кода.
- Отлично работает как общий feature extractor.

**Минусы:**

- Уже не SOTA по качеству/эффективности, но в условиях тура это не критично.

---

### 1.2 EfficientNet (B0–B7) — когда важна эффективность

**Идея:** compound scaling (глубина/ширина/разрешение масштабируются согласованно). Выше качество за те же FLOPs по сравнению с «старыми» CNN.

Базовые чекпоинты на HF: :contentReference[oaicite:3]{index=3}  

- `google/efficientnet-b0` — маленький, очень удобный как «универсальный» backbone.  
- `google/efficientnet-b3` — заметно сильнее, но тяжелее.  
- Остальные `google/efficientnet-b1…b7` — по ситуации, но на туре чаще всего хватит B0/B3.

**Когда использовать:**

- Нужно **лучше, чем ResNet-18**, при этом сохранять адекватную скорость.
- Ограниченный compute, но важна **точность на картинках** (медицинские, тех. объекты и т.п.).
- Ты хочешь компактный embedding и небольшой размер модели.

**Плюсы:**

- Отличный trade-off качество/скорость.
- Часто выигрывает у ResNet при тех же ресурсах.

**Минусы:**

- Архитектура посложнее для кастомных модификаций.
- Fine-tune может быть чувствительнее к lr/аугментациям.

---

### 1.3 ConvNeXt / ConvNeXtV2 — «современный большой CNN»

**Идея:** «ResNet в духе Vision Transformer» — большие kernel’ы, упрощенная структура, сильное качество.

Полезные модели на HF (для тура имеет смысл брать **tiny/base**): :contentReference[oaicite:4]{index=4}  

- `facebook/convnext-tiny-224`  
- `facebook/convnext-base-224`  
- `facebook/convnext-base-224-22k-1k` — предобучен на ImageNet-22k, затем на 1k.

**Когда использовать:**

- Задача **чисто про картинки**, метрика жёсткая, и есть запас по GPU/времени.
- План: **заморозить ConvNeXt** и использовать как фичи (а не full fine-tune).

**Плюсы:**

- Очень сильные визуальные эмбеддинги.
- Неплохо работают в zero/few-shot сценариях при грамотной голове.

**Минусы:**

- Тяжелее ResNet/EfficientNet — для полного fine-tune дорого по VRAM и времени.
- На туре чаще логично использовать как **замороженный encoder**.

---

### 1.4 Лёгкие MobileNet-подобные (косвенно)

Чистых MobileNet на HF много, но в контексте тура это важно скорее через **готовые модели**, например YamNet (см. ниже). Глубоко в MobileNet-v2/v3 обычно нет смысла залезать, если у тебя нет жёстких ограничений по latency.

---

## 2. Аудио-CNN-бэкбоны

Здесь интереснее, потому что у тебя есть два пути:

1. **Сделать лог-мел спектр → скормить vision-backbone (ResNet/EfficientNet).**
2. **Взять специализированный audio-CNN**, обученный на AudioSet и т.п., и использовать как готовый эмбеддинг.

Второй путь часто сильнее и быстрее для тура.

---

### 2.1 PANNs (CNN6/10/14, Wavegram_Logmel, ResNet22)

PANNs (Pretrained Audio Neural Networks) — крупный набор аудио-CNN, популярный в DCASE/EVAR и др. :contentReference[oaicite:5]{index=5}  

На HF есть зеркала:

- `nicofarr/panns_Cnn14` — самый частый выбор, сильный general-purpose эмбеддер. :contentReference[oaicite:6]{index=6}  
- `nicofarr/panns_Cnn10`, `nicofarr/panns_Cnn6` — поменьше, проще если мало ресурсов. :contentReference[oaicite:7]{index=7}  
- `nicofarr/panns_Wavegram_Logmel_Cnn14` — комбинирует wave+logmel, более тяжелый, но мощный. :contentReference[oaicite:8]{index=8}  
- `nicofarr/panns_ResNet22` — ResNet-вариант PANNs. :contentReference[oaicite:9]{index=9}  

**Как использовать на туре:**

- Использовать как **feature extractor**:
  - Загружаешь модель.
  - Превращаешь звук в лог-мел (если модель ожидает спектр) или в нужное представление.
  - Достаёшь **embedding на клип** (pooling по времени).
  - Дальше — LightGBM/MLP/linear head.

**Когда брать:**

- Классическая **audio-tagging / event classification / multi-label** задача.
- Нужны **качественные эмбеддинги** за короткое время.
- Ты не хочешь городить свой CNN с нуля.

**Плюсы:**

- Обучены на больших датасетах (AudioSet и т.п.), хорошо обобщают.
- Много опыта в соревновательной тусовке — есть понятные паттерны.

**Минусы:**

- Интерфейс менее стандартизирован, чем у vision-моделей в transformers.
- Нужно внимательно смотреть, что модель ожидает на вход (wave vs log-mel).

---

### 2.2 VGGish — старый, но полезный baseline

Порт Google VGGish на HF:

- `niobures/VGGish` или `thelou1s/viggish` (копии TFHub-версии). :contentReference[oaicite:10]{index=10}  

**Идея:**

- Это VGG-подобный CNN, обученный на AudioSet для эмбеддингов 128-размерности.
- Часто используется как **универсальный аудио-frontend**.

**Когда использовать на туре:**

- Нужен **быстрый и очень простой baseline** по звуку.
- Ты хочешь «попробовать хоть что-то предобученное» и дальше бросать табличку.

**Плюсы:**

- Максимально простой pipeline, много примеров.
- Маленький embedding → удобно для бутстрэп-анализа, быстрых моделей.

**Минусы:**

- Уже сильно не SOTA, PANNs/CLAP/AST обычно лучше.
- Но для тура как baseline ок.

---

### 2.3 YamNet — MobileNet-CNN для аудио событий

**YamNet** — CNN на основе MobileNet-v1, обученный на AudioSet, заточен под audio event classification. :contentReference[oaicite:11]{index=11}  

На HF есть несколько реализаций:

- `thelou1s/yamnet` — копия TFHub-версии. :contentReference[oaicite:12]{index=12}  
- `STMicroelectronics/yamnet` — адаптация под embedded, даёт эмбеддинги 256-д. :contentReference[oaicite:13]{index=13}  
- `qualcomm/YamNet` — заточен под Qualcomm-девайсы, но это тот же YamNet-бэкбон. :contentReference[oaicite:14]{index=14}  

**Когда использовать:**

- Короткие клипы, задача — **детектировать тип события** (шум, речь, лай собаки, сигнал и т.п.).
- Нужен **очень лёгкий и быстрый** энкодер (мало параметров, быстрый inference).
- У тебя может быть много коротких фрагментов, и важна скорость обработки.

**Плюсы:**

- Лёгкий (порядка нескольких М параметров).
- Хорошие эмбеддинги для событийного аудио.

**Минусы:**

- Не самый сильный по качеству на сложных задачах; для «музыка/речь/эмоции» PANNs/CLAP часто лучше.

---

### 2.4 Гибриды с CNN-фронтендом: CLAP HTSAT (упоминание)

Формально это уже не «чистый CNN-backbone», но полезно знать, что есть:

- `laion/clap-htsat-unfused` — аудио-языковая модель (CLAP), в которой спектрограмма прогоняется через HTSAT (CNN+transformer-подобный энкодер). :contentReference[oaicite:15]{index=15}  

Для тура это актуально, если дадут **аудио + текст** (описания классов, теги) и нам нужен **zero-shot или мультимодальный** подход. Но это уже больше про contrastive/transformer, а не про «чистый CNN».

---

## 3. Как выбирать backbone на туре (быстрый чек-лист)

### 3.1 Картинки (или спектрограммы как 2D-картинки)

**Если мало времени и хочется стабильный baseline:**

- `microsoft/resnet-18` или `microsoft/resnet-50`  
- Построить head: GlobalAvgPool → Linear/MLP.  
- При недостатке данных — фризить большую часть слоёв.

**Если упираешься в качество, но ресурсы позволяют:**

- `google/efficientnet-b0` для начала.  
- Если совсем нужно выжать всё — `google/efficientnet-b3` или `facebook/convnext-tiny-224`.  

**Если хочешь использовать CNN как embedding-extractor для таблички:**

1. Готовый CNN (ResNet/EfficientNet).
2. Прогоняешь все train/test картинки → embedding-векторы (например, 512/2048-д).
3. На этих embedding’ах уже катаешь LightGBM/MLP и обычный CV/OOF.

---

### 3.2 Аудио

**Сценарий 1: нужен сильный baseline без качелей**

- Берёшь `nicofarr/panns_Cnn14` как encoder.  
- Делаешь лог-мел, прогоняешь, усредняешь по времени → 2048-д фича. :contentReference[oaicite:16]{index=16}  
- Поверх: LightGBM/MLP/логрег.

**Сценарий 2: нужен лёгкий и быстрый**

- `STMicroelectronics/yamnet` или `thelou1s/yamnet` → 256/1024-д эмбеддинг. :contentReference[oaicite:17]{index=17}  

**Сценарий 3: совсем простой baseline**

- `niobures/VGGish` как frontend, дальше табличка. :contentReference[oaicite:18]{index=18}  

**Сценарий 4: у тебя уже готов свой CNN (как мы писали)**

- Используешь свой `AudioCNN` на лог-мел.  
- Если по CV он проигрывает PANNs/YamNet — можно комбинировать:  
  - свой CNN + PANNs embedding → конкат → head.

---

## 4. Микро-шпаргалка «что взять по умолчанию»

- **Картинки / спектрограммы, baseline:**  
  → `microsoft/resnet-18` или `google/efficientnet-b0`.

- **Картинки, упор на качество:**  
  → `google/efficientnet-b3` или `facebook/convnext-tiny-224` в замороженном режиме.

- **Аудио, нормальный compute, хочется «правильно»:**  
  → `nicofarr/panns_Cnn14` как embedding-extractor.

- **Аудио, нужен лёгкий и быстрый encoder:**  
  → `STMicroelectronics/yamnet` / `thelou1s/yamnet`.

- **Аудио, совсем базовый baseline:**  
  → `niobures/VGGish`.

Главное — не зарываться в экзотику. На туре выигрывает не редкий архитектурный зверь, а предсказуемый пайплайн, который ты можешь успеть настроить и репродьюсить.

---
