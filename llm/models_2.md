Ок, давай это разложим уже не по «семействам», а по **типам задач**: классификация, QA, извлечение, ранжирование, саммари и т.д.
Буду сразу писать: **какие типы моделей**, **конкретные примеры на HF**, и **когда что юзать на туре**.

> Кстати: часть файлов, которые ты загружал в прошлых сессиях (PDF/ноуты), сейчас уже недоступна — если захочешь, чтобы я снова к ним отослался, их надо будет перезалить.

---

## 1. Классификация / регрессия по тексту (toxicity, intent, topics, score)

### Вариант A: классические encoder-модели (когда есть train и время дообучать)

**Когда так делать:**

* Есть **размеченный train**, задача — стабильная метрика (AUC/F1/accuracy/RMSE).
* Текст умеренной длины (до пары абзацев).
* Важна **детерминированность и скорость** (гоняешь много CV-сплитов).

**Модели:**

* Англ / общие:

  * `microsoft/deberta-v3-base` / `large` — очень сильная база для классификации.
  * `roberta-large` / `roberta-base` — старый, но надёжный боевой конь.
* Мультиязычные:

  * `xlm-roberta-base` / `xlm-roberta-large` — если есть русский + ещё языки.
  * `sentence-transformers/paraphrase-multilingual-mpnet-base-v2` — как encoder для фичей (потом LightGBM/MLP).
  * `intfloat/multilingual-e5-base` — для задач «текст → embedding → табличка/nearest neighbors».

**Как использовать на туре:**

* Классика: `AutoModelForSequenceClassification` + CV, или encoder → pooling → MLP/LightGBM.
* Хороший паттерн: сделать **две ветки**:

  1. Encoder + linear head (HuggingFace).
  2. Encoder → embedding → LightGBM/CatBoost.

Часто 2-й вариант даёт стабильнее OOF за счёт табличной магии.

---

### Вариант B: LLM как классификатор (few-shot / zero-shot)

**Когда так делать:**

* Лейблы «словесные» (например, «токсичный / вежливый / саркастичный»).
* Тренить нет времени, а метрика даёт шанс на **few-shot prompting**.
* Часто в генеративных задачах Всероса — «прочитай и верни label/JSON».

**Модели:**

* Основной мозг:

  * `meta-llama/Llama-3.1-8B-Instruct` (FP4/FP8 квант для экономии VRAM).
  * `google/gemma-2-9b-it` (INT4/FP8 / GGUF).
  * `mistralai/Mistral-7B-Instruct-v0.3`.
* Маленький tool-LLM:

  * `microsoft/Phi-3-mini-4k-instruct` — идеально для «прочитай текст → верни один из N классов / JSON».

**Паттерн:**

* В промпт:

  * чёткое описание задачи,
  * список допустимых классов с описаниями,
  * несколько размеченных примеров (few-shot),
  * строгий формат ответа (`{"label": "TOXIC"}`).
* Сверху — **validator** по JSON + маппер строк → id класса.

---

## 2. QA: SQuAD-подобные, Multiple Choice, free-form

### 2.1 Extractive QA (спаны, SQuAD-стиль)

**Когда так делать:**

* Дают контекст + вопрос, нужно **отрезать спан текста**, метрика EM/F1 по токенам.
* Типичный формат «из этого абзаца вырежи точный ответ».

**Модели:**

* `deepset/roberta-base-squad2` или любые `*-squad` (англ).
* Мультияз — хуже покрыт, но можно брать `xlm-roberta-large` и дообучать под `question-answering`.

**Использование:**

* `AutoModelForQuestionAnswering`: вход — `[CLS] question [SEP] context [SEP]`.
* Выход: start/end logits → спан.
* Очень удобно для задач типа «номер строки / конкретное число / слово».

---

### 2.2 Multiple-Choice QA (варианты ответов)

**Когда:**

* Формат «вопрос + 4 опции».
* Метрика accuracy / logloss по выбору варианта.

**Варианты:**

1. **Encoder-классификация:**

   * Склеить `question + option_i` → encoder → классифицировать индекс i.
   * В паре с `deberta-v3-base` / `roberta-large` / `xlm-roberta-large`.

2. **LLM-логика:**

   * LLM (Llama/Gemma/Mistral) получает вопрос и опции, возвращает:

     * либо букву (`"C"`),
     * либо рейтинг опций (1–5).
   * Можно делать self-consistency: прогнать несколько раз с разными order/shuffle.

**Когда что:**

* Если есть train — encoder + классификатор.
* Если train мало / сложные reasoning вопросы — LLM с chain-of-thought + voting.

---

### 2.3 Open QA / RAG QA (длинные документы)

**Когда:**

* Доков много, вопросы разные, модель должна:

  * вытащить релевантные куски (retrieval),
  * прочитать их и ответить,
  * иногда с цитатами.

**Модели:**

* Ретривер:

  * `BAAI/bge-base-en-v1.5`, `BAAI/bge-m3` — очень сильные би-энкодеры.
  * `intfloat/e5-base-v2` / `multilingual-e5-base` для мультиязыка.
* LLM-голова (генератор):

  * `meta-llama/Llama-3.1-8B-Instruct`.
  * `google/gemma-2-9b-it`.
  * `mistralai/Mistral-7B-Instruct-v0.3`.

**Когда какую связку:**

* Если контекст короткий, а вопросов много → важно, чтобы retriever был топовый → бери bge/e5.
* Если текст на нескольких языках (рус/англ) → multilingual-e5 + Llama/Gemma (они мультиязычные сами по себе).

---

## 3. Извлечение, NER, структурный JSON

Это то, что очень любят во всерос-LLM:
«прочитай текст → вытащи сущности / отношения / события → верни строго по схеме».

### 3.1 Token-level (классический NER)

**Когда:**

* Есть много разметки по токенам.
* Набор типов фиксирован: PER/ORG/LOC/DATE/…
* Метрика token/char-level F1.

**Модели:**

* `dslim/bert-base-NER` (англ).
* Любой `roberta-base` / `deberta-v3-base` / `xlm-roberta-base` → `AutoModelForTokenClassification`.

**Плюсы:**

* Супер-строгий контроль над спанами.
* Легко считать F1/precision/recall.

**Минусы:**

* Не супер-гибко, если схема сложная (nested entities, сложные отношения).

---

### 3.2 LLM → JSON (schema-guided extraction)

**Когда:**

* Нужно не просто NER, а полноценный **граф / JSON-структура**:

  * сущности + атрибуты,
  * связи (agent, patient, cause, time),
  * нормализованные значения (например, даты, валюты, id-и и т.д.).

**Модели:**

* Основной worker:

  * Llama 3.1 / Gemma 2 / Mistral 7B — как «мозг» для parsing-а.
* Спец:

  * `EmergentMethods/Phi-3-mini-4k-instruct-graph` — мелкий, заточен на entity-relationship extraction, удобно как **второй шаг**, который превращает черновой текст → граф.

**Паттерн:**

1. Πишешь **чёткую JSON-схему** в промпт, с примерами.
2. Гоняешь LLM → получаешь candidate JSON.
3. Пропускаешь через **валидатор** (pydantic / jsonschema):

   * если ок — принимаешь,
   * если нет — даёшь модели текст ошибки и просишь «исправь, но не меняй смысл».

**Когда это лучше NER:**

* Когда сущности сериализуются в сложные объекты (list-ы, вложенные структуры).
* Когда нужно сразу выдать то, что потом пойдёт в метрику/постпроцесс, а не просто BIO-теги.

---

## 4. Ранжирование и семантическая близость (pair scoring, rerank)

### 4.1 Bi-encoder + ANN (retrieval / matching)

**Когда:**

* Нужно находить похожие тексты / документы / items.
* Метрики типа Recall@K, mAP@K, NDCG @ K.

**Модели:**

* `sentence-transformers/all-MiniLM-L6-v2` — маленький, быстрый baseline для англ.
* `BAAI/bge-base-en-v1.5`, `BAAI/bge-small-en-v1.5`, `BAAI/bge-m3` — SOTA-ish, разные размеры.
* `intfloat/multilingual-e5-base` — мультиязычные эмбеддинги.

**Использование:**

* Тексты → embedding → FAISS / HNSW index → top-K кандидатов.
* Дальше можно подмешивать BM25 или табличку.

---

### 4.2 Cross-encoder / LLM-reranker

**Когда:**

* Кандидаты уже есть, нужно **тонко отличить** качество:

  * ранжирование ответов в RAG,
  * оценка похожести question–answer / query–document,
  * re-rank выдачи поиска/рекомендаций.

**Модели:**

* Готовые cross-encoders:

  * `cross-encoder/ms-marco-MiniLM-L-6-v2`.
  * `cross-encoder/ms-marco-MultiBERT-L-12` (multi-lingual).
* LLM как reranker:

  * Llama / Gemma / Mistral:

    * даёшь список кандидатов,
    * просишь: «оцени каждый по шкале 0–10 как ответ на запрос X».
    * сортируешь по score.

**Когда что:**

* Если времени и ресурсов мало — bi-encoder-only.
* Если RAG-метрика чувствительна к точности топ-1 — добавляешь LLM rerank хотя бы на top-20.

---

## 5. Саммаризация, переформулировка, генерация

**Когда:**

* Нужна хорошая **вера к источнику** (faithful summary).
* Задача: TL;DR, объясни своими словами, переведи в другую форму.

**Модели:**

* Основные:

  * Llama 3.1 8B Instruct.
  * Gemma 2 9B IT.
  * Mistral 7B Instruct.
* Маленькие:

  * Phi-3-mini — для коротких текстов / множества маленьких запросов.

**Паттерн на туре:**

* Если есть лимит контекста:

  * chunk’ишь документ → частичные саммари → recurse (map-reduce).
* Для задач типа «объясни решение / выведи план»:

  * сначала модель решает и комментирует chain-of-thought (внутри),
  * потом ты просишь её выдать **короткий внешний ответ**, без лишних рассуждений.

---

## 6. Код и «строгая логика» (если вдруг понадобится)

Иногда в задачах появляются:

* «сгенерируй кусок кода / SQL / DSL на основе описания»,
* «исправь формулу / условие».

Тут можно иметь **узкоспециализированный кодер**:

* `deepseek-ai/deepseek-coder-6.7b-instruct`.
* `Qwen2.5-Coder-7B-Instruct`.

Но часто проще использовать ту же Llama/Gemma/Mistral: они и так довольно сильны в коде, а меньше переусложнения пайплайна.

---

## 7. Как выбирать «по ситуации» на туре

Чтобы это не было просто списком моделей, даю маленькую матрицу мыслей:

1. **Есть разметка + нужно честное CV →**
   → бери **encoder (DeBERTa/XLM-R)** и дообучай под:

   * sequence classification (toxicity, topics),
   * token classification (NER),
   * QA (SQuAD-style).

2. **Разметки мало / динамические label’ы / сложная логика →**
   → играем через **LLM + промпты**:

   * Мозг: Llama / Gemma / Mistral.
   * Tool: Phi-3-mini для JSON/классификаций/repair.

3. **RAG/QA по документам →**

   * Ретривер: BGE / e5 / ST-model.
   * Генератор: Llama/Gemma/Mistral.
   * Плюс LLM-rerank, если важен топ-1 ответ.

4. **Жёсткий JSON / граф / извлечение сущностей →**

   * LLM c очень чётким schema-промптом.
   * В идеале: маленький `Phi-3-mini-graph` как спец-мозг для graph extraction.
   * Поверх — валидатор + repair-loop.

5. **Надо много маленьких LLM-вызовов (self-consistency, scoring, classification) →**

   * главный большой LLM выключаем,
   * берём `Phi-3-mini-4k-instruct` (4bit/GGUF) и бомбим им.

---

Если хочешь, следующим шагом могу сделать именно **табличку под олимпиаду**:

* строки — тип задачи (toxicity, NER, span QA, RAG QA, JSON extraction, rerank, summarization),
* столбцы — «если есть train», «если train мало», «LLM-режим»,
* а в ячейках — 1–2 конкретных HF-модели + короткий паттерн пайплайна.
